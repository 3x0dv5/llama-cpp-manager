{
  "system": {
    "cpu_cores_physical": 16,
    "cpu_cores_logical": 32,
    "ram_total_mb": 128726,
    "gpu_type": "CUDA",
    "gpu_vram_mb": 24564
  },
  "software": {
    "binary_path": "/home/u/.local/bin/llama-cli",
    "binary_type": "CPU"
  },
  "models": [
    {
      "source": "local",
      "path": "/home/u/.cache/llama.cpp/unsloth_Qwen3-Coder-Next-GGUF_Qwen3-Coder-Next-Q4_K_M.gguf",
      "size_mb": 46259,
      "alias": "unsloth_Qwen3-Coder-Next-GGUF_Qwen3-Coder-Next-Q4_K_M.gguf",
      "is_embedding": false,
      "is_reranker": false
    },
    {
      "source": "local",
      "path": "/home/u/.cache/llama.cpp/Qwen_Qwen3-Embedding-8B-GGUF_Qwen3-Embedding-8B-Q4_K_M.gguf",
      "size_mb": 4460,
      "alias": "Qwen_Qwen3-Embedding-8B-GGUF_Qwen3-Embedding-8B-Q4_K_M.gguf",
      "is_embedding": true,
      "is_reranker": false
    },
    {
      "source": "local",
      "path": "/home/u/.cache/llama.cpp/mradermacher_Qwen3-Reranker-8B-GGUF_Qwen3-Reranker-8B.Q4_K_M.gguf",
      "size_mb": 4793,
      "alias": "mradermacher_Qwen3-Reranker-8B-GGUF_Qwen3-Reranker-8B.Q4_K_M.gguf",
      "is_embedding": false,
      "is_reranker": true
    }
  ],
  "server_params": {
    "--threads": 15,
    "--embeddings": true,
    "--n-gpu-layers": 0,
    "--ctx-size": 4096,
    "--batch-size": 256,
    "--mlock": true
  },
  "rationale": [
    "Threads: 15 (Physical cores 16 - 1 reserved)",
    "Embeddings: Enabled (Detected embed/bert in model name)",
    "GPU Support: Hardware has CUDA, but llama-cli was compiled for CPU only. Forcing CPU mode.",
    "GPU Offload: None (CPU only)",
    "Context: 4096 (Safe default)",
    "Batch Size: 256 (CPU optimized)",
    "Memory Lock: Enabled (Sys RAM 128726MB >> Models 55512MB)"
  ]
}