{
  "system": {
    "cpu_cores_physical": 12,
    "cpu_cores_logical": 24,
    "ram_total_mb": 128729,
    "gpu_type": "CUDA",
    "gpu_vram_mb": 24564
  },
  "software": {
    "binary_path": "/home/u/.local/bin/llama-cli",
    "binary_type": "CPU"
  },
  "models": [
    {
      "source": "local",
      "path": "-hf unsloth/Qwen3-Coder-Next-GGUF:Q4_K_M",
      "size_mb": 0,
      "alias": "Qwen3-Coder-Next-GGUF:Q4_K_M",
      "is_embedding": false,
      "is_reranker": false
    },
    {
      "source": "local",
      "path": "-hf Qwen/Qwen3-Embedding-8B-GGUF:Q4_K_M",
      "size_mb": 0,
      "alias": "Qwen3-Embedding-8B-GGUF:Q4_K_M",
      "is_embedding": true,
      "is_reranker": false
    },
    {
      "source": "local",
      "path": "-hf mradermacher/Qwen3-Reranker-8B-GGUF:Q4_K_M",
      "size_mb": 0,
      "alias": "Qwen3-Reranker-8B-GGUF:Q4_K_M",
      "is_embedding": false,
      "is_reranker": true
    }
  ],
  "server_params": {
    "--threads": 11,
    "--embeddings": true,
    "--n-gpu-layers": 0,
    "--ctx-size": 4096,
    "--batch-size": 256,
    "--mlock": true
  },
  "rationale": [
    "Threads: 11 (Physical cores 12 - 1 reserved)",
    "Embeddings: Enabled (Detected embed/bert in model name)",
    "GPU Support: Hardware has CUDA, but llama-cli was compiled for CPU only. Forcing CPU mode.",
    "GPU Offload: None (CPU only)",
    "Context: 4096 (Safe default)",
    "Batch Size: 256 (CPU optimized)",
    "Memory Lock: Enabled (Sys RAM 128729MB >> Models 0MB)"
  ]
}